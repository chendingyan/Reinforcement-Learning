{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,0), (3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (2,2)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [1.0, 0.0, 0.0, 0.0]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "#     def value_iteration(self, discount, threshold, probability):\n",
    "#         V = np.zeros(self.state_size)\n",
    "        \n",
    "#         T = self.get_transition_matrix()\n",
    "#         R = self.get_reward_matrix()\n",
    "        \n",
    "#         epochs = 0\n",
    "#         while True:\n",
    "#             epochs+=1\n",
    "#             delta = 0\n",
    "\n",
    "#             for state_idx in range(self.state_size):\n",
    "#                 if(self.absorbing[0,state_idx]):\n",
    "#                     continue \n",
    "                    \n",
    "#                 v = V[state_idx]\n",
    "\n",
    "#                 Q = np.zeros(4)\n",
    "#                 for state_idx_prime in range(self.state_size):\n",
    "#                     if state_idx_prime == 0:\n",
    "#                         first_prime = 1\n",
    "#                         second_prime = 2\n",
    "#                         third_prime = 3\n",
    "#                     elif state_idx_prime == 1:\n",
    "#                         first_prime = 0\n",
    "#                         second_prime = 2\n",
    "#                         third_prime = 3\n",
    "#                     elif state_idx_prime == 2:\n",
    "#                         first_prime = 0\n",
    "#                         second_prime = 1\n",
    "#                         third_prime = 3  \n",
    "#                     elif state_idx_prime == 3:\n",
    "#                         first_prime = 0\n",
    "#                         second_prime = 1\n",
    "#                         third_prime = 2\n",
    "#                     Q += probability * (T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])) +\\\n",
    "#                     (1-probability)/3 * (T[first_prime,state_idx,:] * (R[first_prime,state_idx, :] + discount * V[first_prime]))+\\\n",
    "#                     (1-probability)/3 * (T[second_prime,state_idx,:] * (R[second_prime,state_idx, :] + discount * V[second_prime]))+\\\n",
    "#                     (1-probability)/3 * (T[third_prime,state_idx,:] * (R[third_prime,state_idx, :] + discount * V[third_prime]))\n",
    "\n",
    "#                 V[state_idx]= np.max(Q)\n",
    "#                 delta = max(delta,np.abs(v - V[state_idx]))\n",
    "#             if(delta<threshold):\n",
    "#                 optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "#                 for state_idx in range(self.state_size):\n",
    "#                     Q = np.zeros(4)\n",
    "#                     for state_idx_prime in range(self.state_size):\n",
    "#                         if state_idx_prime == 0:\n",
    "#                             first_prime = 1\n",
    "#                             second_prime = 2\n",
    "#                             third_prime = 3\n",
    "#                         elif state_idx_prime == 1:\n",
    "#                             first_prime = 0\n",
    "#                             second_prime = 2\n",
    "#                             third_prime = 3\n",
    "#                         elif state_idx_prime == 2:\n",
    "#                             first_prime = 0\n",
    "#                             second_prime = 1\n",
    "#                             third_prime = 3  \n",
    "#                         elif state_idx_prime == 3:\n",
    "#                             first_prime = 0\n",
    "#                             second_prime = 1\n",
    "#                             third_prime = 2\n",
    "#                         Q += probability * (T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])) +\\\n",
    "#                         (1-probability)/3 * (T[first_prime,state_idx,:] * (R[first_prime,state_idx, :] + discount * V[first_prime]))+\\\n",
    "#                         (1-probability)/3 * (T[second_prime,state_idx,:] * (R[second_prime,state_idx, :] + discount * V[second_prime]))+\\\n",
    "#                         (1-probability)/3 * (T[third_prime,state_idx,:] * (R[third_prime,state_idx, :] + discount * V[third_prime]))\n",
    "                        \n",
    "                    \n",
    "#                     optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "#                 return V, optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "#     def policy_iteration(self, discount=0.9, threshold = 0.0001):\n",
    "#         policy= np.zeros((self.state_size, self.action_size))\n",
    "#         policy[:,0] = 1\n",
    "        \n",
    "#         T = self.get_transition_matrix()\n",
    "#         R = self.get_reward_matrix()\n",
    "        \n",
    "#         epochs =0\n",
    "#         while True: \n",
    "#             V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "#             epochs+=epochs_eval\n",
    "#             #Policy iteration\n",
    "#             policy_stable = True\n",
    "            \n",
    "#             for state_idx in range(policy.shape[0]):\n",
    "#                 if(self.absorbing[0,state_idx]):\n",
    "#                     continue \n",
    "                    \n",
    "#                 old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "#                 Q = np.zeros(4)\n",
    "#                 for state_idx_prime in range(policy.shape[0]):\n",
    "#                     Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                \n",
    "#                 new_policy = np.zeros(4)\n",
    "#                 new_policy[np.argmax(Q)]=1\n",
    "#                 policy[state_idx] = new_policy\n",
    "                \n",
    "#                 if(old_action !=np.argmax(policy[state_idx])):\n",
    "#                     policy_stable = False\n",
    "            \n",
    "#             if(policy_stable):\n",
    "#                 return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount, probability):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        \n",
    "                        tmpQ = tmpQ + probability* (T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime]))+\\\n",
    "                        (1-probability)/3 * (T[state_idx_prime,state_idx,(action_idx+1)%4] * (R[state_idx_prime,state_idx, (action_idx+1)%4] + discount * V[state_idx_prime]))+\\\n",
    "                        (1-probability)/3 * (T[state_idx_prime,state_idx,(action_idx+2)%4] * (R[state_idx_prime,state_idx, (action_idx+2)%4] + discount * V[state_idx_prime]))+\\\n",
    "                        (1-probability)/3 * (T[state_idx_prime,state_idx,(action_idx+3)%4] * (R[state_idx_prime,state_idx, (action_idx+3)%4] + discount * V[state_idx_prime]))\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch, policy\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    \n",
    "                \n",
    "                        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "[  0.           1.74999375  -1.00003125  -1.0001       1.7500625\n",
      "  -0.9999625   -1.0001      -1.00071875 -25.75069375  -1.00071875\n",
      "   0.        ]\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "discount = 20\n",
    "theta = 0.0001\n",
    "probability = 0.4\n",
    "V, epochs,policy = grid.policy_evaluation(Policy, discount, theta, probability)\n",
    "print(V)\n",
    "# print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "\n",
    "# val, epochs = grid.policy_evaluation(Policy,0.001,0.9)\n",
    "# print(\"The value of that policy is :{}\".format(val))\n",
    "# print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "# gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "# epochs_needed = []\n",
    "# for gamma in gamma_range:\n",
    "#     val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "#     epochs_needed.append(epochs)\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(gamma_range,epochs_needed)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "# print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "# print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "# print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "# pol_opt2, epochs = grid.value_iteration()\n",
    "# print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "# print(\"The number of epocs for convergence is {}\".format(epochs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "# Policy2 = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "# grid.draw_deterministic_policy(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_V(V):  \n",
    "    count = 0\n",
    "    for i in range(16):\n",
    "        if i in [6,8,12,13,15]:\n",
    "            print'{0:>6.2f}'.format(0.0),\n",
    "        else:\n",
    "            print'{0:>6.2f}'.format(V[count]),\n",
    "            count +=1\n",
    "        if (i + 1) % 4 == 0:\n",
    "            print(\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_V(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
